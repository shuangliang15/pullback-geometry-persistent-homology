{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import math, pandas, pickle\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import gudhi as gd\n",
    "from gudhi.weighted_rips_complex import WeightedRipsComplex\n",
    "from gudhi.dtm_rips_complex import DTMRipsComplex\n",
    "from ripser import ripser\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from gudhi.tensorflow import RipsLayer\n",
    "import plotly.graph_objects as go\n",
    "import ot.plot\n",
    "import matplotlib.pylab as pl\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "from utilyze import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_align(source_pc, target_pc, output_icp = False, max_correspondence_distance=.1):\n",
    "    source_cloud = o3d.geometry.PointCloud()\n",
    "    source_cloud.points = o3d.utility.Vector3dVector(source_pc)\n",
    "    target_cloud = o3d.geometry.PointCloud()\n",
    "    target_cloud.points = o3d.utility.Vector3dVector(target_pc)\n",
    "    # Perform ICP\n",
    "    icp_result = o3d.pipelines.registration.registration_icp(\n",
    "        source_cloud,\n",
    "        target_cloud,\n",
    "        max_correspondence_distance=max_correspondence_distance,\n",
    "        estimation_method = o3d.pipelines.registration.TransformationEstimationPointToPoint()\n",
    "    )\n",
    "    # Get the aligned source cloud\n",
    "    aligned_source_cloud = source_cloud.transform(icp_result.transformation)\n",
    "    pc1_regis = np.asarray(aligned_source_cloud.points)\n",
    "    if output_icp:\n",
    "        output = (pc1_regis, icp_result)\n",
    "    else:\n",
    "        output = pc1_regis\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original pointcloud size: (116361, 3)\n",
      "Randomness check: [88916 11866 12750 14216 86734] [88916 11866 12750 14216 86734]\n",
      "Number of data:  96\n",
      "Sampled pc size:  (2000, 3)\n"
     ]
    }
   ],
   "source": [
    "random_seed()\n",
    "# Raw data is downloaded from '# https://gitlab.com/alexpieloch/PersistentHomologyAnalysisofBrainArteryTrees/-/tree/master/data/OriginalBrainTreeData'\n",
    "data_pc = np.load(\"Data/data_pc.pkl\", allow_pickle=True)\n",
    "print('Original pointcloud size:',data_pc[0].shape)\n",
    "# Subsample\n",
    "number_pts = 2000 \n",
    "data_pc_subsample = []\n",
    "for i in range(len(data_pc)):\n",
    "    if i not in [40, 45]:\n",
    "        pc = data_pc[i]\n",
    "        all_idx = np.random.permutation(pc.shape[0])\n",
    "        for j in range(1):\n",
    "            small_idx = all_idx[int(j * number_pts) : int((j+1) * number_pts)]\n",
    "            pc_sub = pc[small_idx]\n",
    "            # normalize\n",
    "            centered_pc_sub = np.copy(pc_sub)\n",
    "            centered_pc_sub[:,0] -= np.mean(pc_sub[:,0])\n",
    "            centered_pc_sub[:,1] -= np.mean(pc_sub[:,1])\n",
    "            centered_pc_sub[:,2] -= np.mean(pc_sub[:,2])\n",
    "            data_pc_subsample.append(centered_pc_sub)\n",
    "data_pc = data_pc_subsample\n",
    "\n",
    "# Normalize\n",
    "mean_max = np.mean([np.max(abs(data_pc[i]),axis=0) for i in range(len(data_pc))])\n",
    "for i in range(len(data_pc)):\n",
    "    data_pc[i] /= mean_max * 2\n",
    "print('Randomness check: [88916 11866 12750 14216 86734]',  all_idx[:5])\n",
    "print('Number of data: ',len(data_pc))\n",
    "print('Sampled pc size: ',data_pc[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the pc-wise Was_icp distance -> interate every i, find the smallest, compute the gradient, subsample 500 pts and gradvec correspondingly\n",
    "n = len(data_pc)\n",
    "distance_matrix = np.zeros((n, n))\n",
    "# Compute the pairwise Wasserstein distances between point clouds\n",
    "for i in range(n):\n",
    "    start_time = time.time()  # Start measuring the time\n",
    "    for j in range(i, n):\n",
    "        pc1,pc2 = data_pc[i],data_pc[j]\n",
    "        pc1_regis = find_align(pc1, pc2)\n",
    "        # Compute wass\n",
    "        M = ot.dist(pc1_regis, pc2)\n",
    "        # Emprical distribution\n",
    "        d1,d2 = np.ones((number_pts,)) / number_pts, np.ones((number_pts,)) / number_pts\n",
    "        # OT matrix\n",
    "        emd = ot.emd(d1,d2,M, numItermax=1e6) #default numItermax 1e5\n",
    "        # Was distance\n",
    "        W = np.sum(emd * M)\n",
    "        distance_matrix[i][j] = W\n",
    "        distance_matrix[j][i] = W\n",
    "    break\n",
    "\n",
    "# import pickle\n",
    "# with open(\"/content/drive/MyDrive/ICG_WAS_DISTMAT.pkl\", 'wb') as f:\n",
    "#     pickle.dump(distance_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('ICG_WAS_DISTMAT.pkl', 'rb') as f:\n",
    "#     distance_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient estimate (finding the neareast data with different lable.)\n",
    "data_grad = []\n",
    "data_pc_real = []\n",
    "number_pts_real = 500\n",
    "\n",
    "df = pandas.read_csv('Data/label.csv')\n",
    "gender = np.array(df['gender'])\n",
    "label = []\n",
    "for i in range(gender.shape[0]):\n",
    "    if i not in [48,50,63,66,69,72,78,84,98,101,103]:\n",
    "        label.append(gender[i])\n",
    "labels_raw=np.array(label)\n",
    "labels_raw -= 1\n",
    "labels = []\n",
    "for i in range(len(labels_raw)):\n",
    "    if i not in [40, 45]:\n",
    "        label_pc = labels_raw[i]\n",
    "        for j in range(1):\n",
    "            labels.append(label_pc)\n",
    "labels = np.array(labels)\n",
    "\n",
    "\n",
    "for i,pc in enumerate(data_pc):\n",
    "    # find the most neareast pc with diff label\n",
    "    label = labels[i]\n",
    "    index_otherclass = np.where(labels!=label)[0]\n",
    "    distances_otherclass = np.array([distance_matrix[i, j] for j in index_otherclass])\n",
    "    j = index_otherclass[np.argmin(distances_otherclass)]\n",
    "    # find the OT vector\n",
    "    pc1 = data_pc[i]\n",
    "    pc2 = data_pc[j]\n",
    "    pc1_regis = find_align(pc1, pc2)\n",
    "    M = ot.dist(pc1_regis, pc2)\n",
    "    n = pc1_regis.shape[0]\n",
    "    d1,d2 = np.ones((n,)) / n, np.ones((n,)) / n\n",
    "    emd = ot.emd(d1,d2,M)\n",
    "    indices = np.argwhere(emd == np.max(emd))\n",
    "    vec = [pc2[l] - pc1_regis[k] for k, l in indices]\n",
    "    vec = np.array(vec)\n",
    "    random_seed()\n",
    "    all_idx = np.random.permutation(pc1_regis.shape[0])\n",
    "    for iter in range(3):\n",
    "        small_idx = all_idx[number_pts_real*iter: number_pts_real*(iter+1)]\n",
    "        pc_real = pc1_regis[small_idx]\n",
    "        vec_real = vec[small_idx]\n",
    "        vec_real = np.array(vec_real).reshape(-1,)\n",
    "        data_grad.append(vec_real)\n",
    "        data_pc_real.append(pc_real)\n",
    "# import pickle\n",
    "# with open(\"ICG_WAS_PC_3.pkl\", 'wb') as f:\n",
    "#     pickle.dump(data_pc_real, f)\n",
    "\n",
    "# with open(\"ICG_WAS_GRAD_3.pkl\", 'wb') as f:\n",
    "#     pickle.dump(data_grad, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ICG_WAS_PC_3.pkl', 'rb') as f:\n",
    "    data_pc = pickle.load(f)\n",
    "data_pc = np.array(data_pc)\n",
    "\n",
    "with open('ICG_WAS_GRAD_3.pkl', 'rb') as f:\n",
    "    data_grad = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute pull-back norms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "xrange = [.0, .25]\n",
    "yrange = [-.05, .2]\n",
    "maximum_edge_length = .25\n",
    "gauss_sigmas = np.linspace(1e-5, 1e-4, 8)\n",
    "number_pixels = [15, 20, 25, 30]\n",
    "\n",
    "Pbn = {}\n",
    "Rank = {}\n",
    "for number_pixel in number_pixels:\n",
    "    Pbn[number_pixel] = {}\n",
    "    Rank[number_pixel] = {}\n",
    "    for gauss_sigma in gauss_sigmas:\n",
    "        Pbn[number_pixel][gauss_sigma] = []\n",
    "        Rank[number_pixel][gauss_sigma] = []\n",
    "\n",
    "\n",
    "random_seed()\n",
    "minisize = len(data_pc)\n",
    "idx = np.random.permutation(len(data_pc))[:minisize]\n",
    "\n",
    "for number_pixel in number_pixels:\n",
    "    for gauss_sigma in gauss_sigmas:\n",
    "        random_seed()\n",
    "        weightfunc = Weight(method='linear', b=yrange[1])\n",
    "        jacobians, pis, ranks = Collect_jacobian_and_rank_and_pi(np.array(data_pc)[idx], get_jacobian_rips, xrange, yrange,\n",
    "                                                number_pixel=number_pixel, gauss_sigma=gauss_sigma, \n",
    "                                                weight_func = weightfunc, max_edge_length=maximum_edge_length, \n",
    "                                                homology_dimensions=[1], normalize=False, normalize_jac=True)\n",
    "        \n",
    "        grads = np.array(data_grad)[idx]\n",
    "        assert len(grads)==len(jacobians)\n",
    "        Pbn[number_pixel][gauss_sigma] = [np.linalg.norm(jacobians[k] @ grads[k]) for k in range(len(grads))]\n",
    "        Rank[number_pixel][gauss_sigma] = ranks\n",
    "        #print(number_pixel)\n",
    "\n",
    "# import pickle\n",
    "# with open(\"Pbn_resvar.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(Pbn, file)\n",
    "\n",
    "# with open(\"Rank_resvar.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(Rank, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downstream performance\n",
    "df = pandas.read_csv('Data/label.csv')\n",
    "gender = np.array(df['gender'])\n",
    "label = []\n",
    "for i in range(gender.shape[0]):\n",
    "    if i not in [48,50,63,66,69,72,78,84,98,101,103]:\n",
    "        label.append(gender[i])\n",
    "labels_raw=np.array(label)\n",
    "#print(np.where(labels_raw==3))\n",
    "#print(np.where(labels_raw==4))\n",
    "#print(len(labels_raw))\n",
    "labels_raw -= 1\n",
    "\n",
    "labels = []\n",
    "for i in range(len(labels_raw)):\n",
    "    if i not in [40, 45]:\n",
    "        label_pc = labels_raw[i]\n",
    "        for j in range(3):\n",
    "            labels.append(label_pc)        \n",
    "labels = np.array(labels)\n",
    "#print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "def train_lr(train_data, train_labels, val_data, val_labels, noise_level = .01, C=1):\n",
    "    random_seed()\n",
    "    lr = LogisticRegression(max_iter=300, C=C) #solver\n",
    "    lr.fit(train_data, train_labels)\n",
    "    test_accuracy = accuracy_score(val_labels, lr.predict(val_data))\n",
    "    test_loss = log_loss(val_labels, lr.predict_proba(val_data))\n",
    "    train_accuracy = accuracy_score(train_labels, lr.predict(train_data))\n",
    "    train_loss = log_loss(train_labels, lr.predict_proba(train_data))\n",
    "\n",
    "    #robust test\n",
    "    val_data_noisy = val_data + np.random.normal(0,noise_level, size = val_data.shape)\n",
    "    robust_test_loss = log_loss(val_labels, lr.predict_proba(val_data_noisy))\n",
    "    robust_test_accuracy = accuracy_score(val_labels, lr.predict(val_data_noisy))\n",
    "\n",
    "    return train_loss, train_accuracy, test_loss, test_accuracy, robust_test_loss, robust_test_accuracy\n",
    "\n",
    "def train_cnn(train_data, train_labels, val_data, val_labels, noise_level=.01, epochs=10, batch_size=32):\n",
    "    random_seed()\n",
    "    # Define the CNN model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(train_data.shape[1], train_data.shape[2], 1)),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)), \n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    # Train the model\n",
    "    history = model.fit(train_data, train_labels,\n",
    "                        validation_data=(val_data, val_labels),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=0)\n",
    "    # Evaluate the model on test data\n",
    "    train_loss, train_accuracy = model.evaluate(train_data, train_labels, verbose=0)\n",
    "    test_loss, test_accuracy = model.evaluate(val_data, val_labels, verbose=0)\n",
    "    # Robust test\n",
    "    val_data_noisy = val_data + np.random.normal(0,noise_level, size = val_data.shape)\n",
    "    robust_test_loss, robust_test_accuracy = model.evaluate(val_data_noisy, val_labels, verbose=0)\n",
    "    return train_loss, train_accuracy, test_loss, test_accuracy, robust_test_loss, robust_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "xrange = [.0, .25]\n",
    "yrange = [-.05, .2]\n",
    "maximum_edge_length = .25\n",
    "\n",
    "\n",
    "gauss_sigmas = np.linspace(1e-5, 1e-4, 8)\n",
    "number_pixels = [15, 20, 25, 30]\n",
    "\n",
    "random_seed()\n",
    "Testacc = {}\n",
    "Robustacc = {}\n",
    "Testacc_cnn = {}\n",
    "Robustacc_cnn = {}\n",
    "\n",
    "for number_pixel in number_pixels:\n",
    "    Testacc[number_pixel] = {}\n",
    "    Robustacc[number_pixel] = {}\n",
    "    Testacc_cnn[number_pixel] = {}\n",
    "    Robustacc_cnn[number_pixel] = {}\n",
    "    for gauss_sigma in gauss_sigmas:\n",
    "        Testacc[number_pixel][gauss_sigma] = []\n",
    "        Robustacc[number_pixel][gauss_sigma] = []\n",
    "        Testacc_cnn[number_pixel][gauss_sigma] = []\n",
    "        Robustacc_cnn[number_pixel][gauss_sigma] = []\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "kf = KFold(n_splits=7)\n",
    "\n",
    "for number_pixel in number_pixels:\n",
    "    for gauss_sigma in gauss_sigmas:\n",
    "        random_seed()\n",
    "        weightfunc = Weight(method='linear', b=yrange[1])\n",
    "        data_pi = Collect_pi(data_pc, xrange, yrange, number_pixel=number_pixel, gauss_sigma=gauss_sigma, weight_func = weightfunc, max_edge_length=maximum_edge_length, homology_dimensions=[1], normalize=False)            \n",
    "        data_pi /= np.max(data_pi) # normalize feature <-> normal jac\n",
    "        \n",
    "        #Logistic\n",
    "        for train_index, test_index in kf.split(data_pi):\n",
    "            train_pi, val_pi = data_pi[train_index], data_pi[test_index]\n",
    "            train_labels, val_labels = labels[train_index], labels[test_index]\n",
    "            train_loss, train_accuracy, test_loss, test_accuracy, robust_test_loss, robust_test_accuracy = train_lr(train_pi, train_labels, val_pi, val_labels, C=1)\n",
    "            Testacc[number_pixel][gauss_sigma].append(test_accuracy)\n",
    "            Robustacc[number_pixel][gauss_sigma].append(robust_test_accuracy)\n",
    "\n",
    "        #CNN\n",
    "        data_image = data_pi.reshape(len(data_pi), number_pixel,number_pixel)\n",
    "        for train_index, test_index in kf.split(data_image):\n",
    "            train_pi, val_pi = data_image[train_index], data_image[test_index]\n",
    "            train_labels, val_labels = labels[train_index], labels[test_index]\n",
    "            train_loss, train_accuracy, test_loss, test_accuracy, robust_test_loss, robust_test_accuracy = train_cnn(train_pi, train_labels, val_pi, val_labels)\n",
    "            Testacc_cnn[number_pixel][gauss_sigma].append(test_accuracy)\n",
    "            Robustacc_cnn[number_pixel][gauss_sigma].append(robust_test_accuracy)\n",
    "        \n",
    "        #print(number_pixel)\n",
    "\n",
    "# import pickle\n",
    "\n",
    "# with open(\"Testacc_resvar.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(Testacc, file)\n",
    "\n",
    "# with open(\"Robustacc_resvar.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(Robustacc, file)\n",
    "\n",
    "# with open(\"Testacc_resvar_cnn.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(Testacc_cnn, file)\n",
    "\n",
    "# with open(\"Robustacc_resvar_cnn.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(Robustacc_cnn, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "dictionaries = [Pbn, Rank]\n",
    "titles = ['Pullback norm','Rank']\n",
    "cmaps = ['Blues','Greys']\n",
    "gauss_sigmas = np.linspace(1e-5, 1e-4, 8)\n",
    "number_pixels = [15, 20, 25, 30]\n",
    "rows = 2\n",
    "cols = 2\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(14, 9))\n",
    "fmts = ['.3f','.1f']\n",
    "gauss_sigmas_label = np.round(gauss_sigmas * 1e5 * 1e2)/1e2\n",
    "FS = 14\n",
    "dictionary = Rank\n",
    "mean = [[np.mean(dictionary[key][sub_key]) for sub_key in gauss_sigmas] for key in number_pixels]\n",
    "mean = np.flip(np.array(mean), axis=0)\n",
    "sd = [[np.std(dictionary[key][sub_key]) for sub_key in gauss_sigmas] for key in number_pixels]\n",
    "sd = np.flip(np.array(sd), axis=0)\n",
    "cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"#ffffff\",\"#252525\"])\n",
    "ax = sns.heatmap(mean, annot=False, cmap=cmap, linewidths=.1, ax=axes[0,0],square=True, cbar=False)\n",
    "\n",
    "ax.set_yticklabels(np.flip(number_pixels, axis=0))\n",
    "ax.set_xticklabels(gauss_sigmas_label)\n",
    "ax.set_title(\"Rank of Jacobian mapping\", fontsize=FS)\n",
    "ax.set_ylabel('Resolution', fontsize=FS)\n",
    "ax.set_xlabel('Variance in Gaussian Kernel (x$10^{-5}$)', fontsize=FS)\n",
    "\n",
    "for i in range(mean.shape[0]):\n",
    "    for j in range(mean.shape[1]):\n",
    "        value = mean[i, j]\n",
    "        std_error = sd[i, j]\n",
    "        annotation_text = \"{:.2f}\\n±{:.2f}\".format(value, std_error)\n",
    "        pixel_color = mean[i][j]\n",
    "        annotation_color = 'black' if pixel_color < 1.1 * np.mean(mean) else 'white'\n",
    "        ax.text(j + 0.5, i + 0.5, annotation_text, ha='center', va='center', color=annotation_color, fontsize=10)\n",
    "dictionary = Pbn\n",
    "mean = [[np.mean(dictionary[key][sub_key]) for sub_key in gauss_sigmas] for key in number_pixels]\n",
    "mean = np.flip(np.array(mean), axis=0)\n",
    "sd = [[np.std(dictionary[key][sub_key]) for sub_key in gauss_sigmas] for key in number_pixels]\n",
    "sd = np.flip(np.array(sd), axis=0)\n",
    "ax = sns.heatmap(mean, annot=False, cmap='Blues', linewidths=.1, ax=axes[0,1],square=True, cbar=False, vmin=.085, vmax=.112)\n",
    "ax.set_yticklabels(np.flip(number_pixels, axis=0))\n",
    "ax.set_xticklabels(gauss_sigmas_label)\n",
    "ax.set_title(\"Pull-back norm of gradient ($ x 10^{-2}$)\", fontsize=FS)\n",
    "ax.set_ylabel('Resolution', fontsize=FS)\n",
    "ax.set_xlabel('Variance of Gaussian Kernel (x$10^{-5}$)', fontsize=FS)\n",
    "for i in range(mean.shape[0]):\n",
    "    for j in range(mean.shape[1]):\n",
    "        value = mean[i, j]*100\n",
    "        std_error = sd[i, j]*100\n",
    "        annotation_text = \"{:.2f}\\n±{:.2f}\".format(value, std_error)\n",
    "        pixel_color = mean[i][j]\n",
    "        annotation_color = 'black' if pixel_color < 1 * np.mean(mean) else 'white'\n",
    "        ax.text(j + 0.5, i + 0.5, annotation_text, ha='center', va='center', color=annotation_color, fontsize=10)\n",
    "###\n",
    "dictionary = Testacc\n",
    "mean = [[ np.mean(dictionary[key][sub_key]) for sub_key in gauss_sigmas] for key in number_pixels]\n",
    "mean = np.flip(np.array(mean), axis=0)\n",
    "sd = [[ np.std(dictionary[key][sub_key]) for sub_key in gauss_sigmas] for key in number_pixels]\n",
    "sd = np.flip(np.array(sd), axis=0)\n",
    "ax = sns.heatmap(mean, annot=False, cmap='Greens', linewidths=.1, ax=axes[1,0], square=True, cbar=False, vmin=.62, vmax=.7)\n",
    "ax.set_yticklabels(np.flip(number_pixels, axis=0))\n",
    "ax.set_title('Validation accuracy (%)', fontsize=FS)\n",
    "ax.set_xticklabels(gauss_sigmas_label)\n",
    "ax.set_ylabel('Resolution', fontsize=FS)\n",
    "ax.set_xlabel('Variance of Gaussian Kernel (x$10^{-5}$)', fontsize=FS)\n",
    "for i in range(mean.shape[0]):\n",
    "    for j in range(mean.shape[1]):\n",
    "        value = mean[i, j] *100\n",
    "        std_error = sd[i, j] *100\n",
    "        annotation_text = \"{:.2f}\\n±{:.2f}\".format(value, std_error)\n",
    "        pixel_color = mean[i][j]\n",
    "        annotation_color = 'black' if pixel_color < 1 * np.mean(mean) else 'white'\n",
    "        ax.text(j + 0.5, i + 0.5, annotation_text, ha='center', va='center', color=annotation_color, fontsize=10)\n",
    "dictionary = Robustacc\n",
    "mean = [[ np.mean(dictionary[key][sub_key]) for sub_key in gauss_sigmas] for key in number_pixels]\n",
    "mean = np.flip(np.array(mean), axis=0)\n",
    "sd = [[ np.std(dictionary[key][sub_key]) for sub_key in gauss_sigmas] for key in number_pixels]\n",
    "sd = np.flip(np.array(sd), axis=0)\n",
    "ax = sns.heatmap(mean, annot=False, cmap='Greens', linewidths=.1, ax=axes[1,1], square=True, cbar=False, vmin=.59, vmax=.7)\n",
    "ax.set_yticklabels(np.flip(number_pixels, axis=0))\n",
    "ax.set_title('Robust validation accuracy (%)', fontsize=FS)\n",
    "ax.set_xticklabels(gauss_sigmas_label)\n",
    "ax.set_ylabel('Resolution', fontsize=FS)\n",
    "ax.set_xlabel('Variance of Gaussian Kernel (x$10^{-5}$)', fontsize=FS)\n",
    "for i in range(mean.shape[0]):\n",
    "    for j in range(mean.shape[1]):\n",
    "        value = mean[i, j] *100\n",
    "        std_error = sd[i, j] *100\n",
    "        annotation_text = \"{:.2f}\\n±{:.2f}\".format(value, std_error)\n",
    "        pixel_color = mean[i][j]\n",
    "        annotation_color = 'black' if pixel_color < 1.0 * np.mean(mean) else 'white'\n",
    "        ax.text(j + 0.5, i + 0.5, annotation_text, ha='center', va='center', color=annotation_color, fontsize=10)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('resvar.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('Testacc_resvar.pkl', \"rb\") as file:\n",
    "    Testacc = pickle.load(file)\n",
    "with open('Robustacc_resvar.pkl', \"rb\") as file:\n",
    "    Robustacc = pickle.load(file)\n",
    "with open('Pbn_resvar.pkl', \"rb\") as file:\n",
    "    Pbn = pickle.load(file)\n",
    "with open('Rank_resvar.pkl', \"rb\") as file:\n",
    "    Rank = pickle.load(file)\n",
    "\n",
    "with open('Testacc_resvar_cnn.pkl', \"rb\") as file:\n",
    "    Testacc_cnn = pickle.load(file)\n",
    "with open('Robustacc_resvar_cnn.pkl', \"rb\") as file:\n",
    "    Robustacc_cnn = pickle.load(file)\n",
    "\n",
    "gauss_sigmas = np.linspace(1e-5, 1e-4, 8)\n",
    "number_pixels = [15, 20, 25, 30]\n",
    "\n",
    "dictionary = Pbn\n",
    "mean = [[np.mean(dictionary[key][sub_key]) for sub_key in gauss_sigmas] for key in number_pixels]\n",
    "acc_list = np.array(mean).reshape(-1)\n",
    "a = acc_list\n",
    "pbn_list = (a - np.min(a)) / (np.max(a) - np.min(a))\n",
    "\n",
    "dictionary = Testacc\n",
    "mean = [[np.mean(dictionary[key][sub_key]) for sub_key in gauss_sigmas] for key in number_pixels]\n",
    "acc_list = np.array(mean).reshape(-1)\n",
    "a = acc_list\n",
    "acc_list = (a - np.min(a)) / (np.max(a) - np.min(a))\n",
    "\n",
    "dictionary = Robustacc\n",
    "mean = [[np.mean(dictionary[key][sub_key]) for sub_key in gauss_sigmas] for key in number_pixels]\n",
    "rob_acc_list = np.array(mean).reshape(-1)\n",
    "a = rob_acc_list\n",
    "rob_acc_list = (a - np.min(a)) / (np.max(a) - np.min(a))\n",
    "\n",
    "dictionary = Rank\n",
    "mean = [[np.mean(dictionary[key][sub_key]) for sub_key in gauss_sigmas] for key in number_pixels]\n",
    "rank_list = np.array(mean).reshape(-1)\n",
    "a = np.array(mean).reshape(-1)\n",
    "rank_list = (a - np.min(a)) / (np.max(a) - np.min(a))\n",
    "\n",
    "dictionary = Testacc_cnn\n",
    "mean = [[np.mean(dictionary[key][sub_key]) for sub_key in gauss_sigmas] for key in number_pixels]\n",
    "a = np.array(mean).reshape(-1)\n",
    "acc_list_cnn = (a - np.min(a)) / (np.max(a) - np.min(a))\n",
    "\n",
    "dictionary = Robustacc_cnn\n",
    "mean = [[np.mean(dictionary[key][sub_key]) for sub_key in gauss_sigmas] for key in number_pixels]\n",
    "a = np.array(mean).reshape(-1)\n",
    "rob_acc_list_cnn = (a - np.min(a)) / (np.max(a) - np.min(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "correlation_coeff, p_value = pearsonr(pbn_list, acc_list)\n",
    "print(correlation_coeff, p_value)\n",
    "\n",
    "correlation_coeff, p_value = pearsonr(pbn_list, rob_acc_list)\n",
    "print(correlation_coeff, p_value)\n",
    "\n",
    "correlation_coeff, p_value = pearsonr(rank_list, acc_list)\n",
    "print(correlation_coeff, p_value)\n",
    "\n",
    "correlation_coeff, p_value = pearsonr(rank_list, rob_acc_list)\n",
    "print(correlation_coeff, p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "fig, axs = plt.subplots(1,4,figsize=(20,5))\n",
    "\n",
    "sns.kdeplot(x=rank_list, y=acc_list,fill=True,ax=axs[0],color='gray',thresh=.05)\n",
    "axs[0].set_xlabel('Rank of Jacobian \\n (a)',fontsize=15)\n",
    "axs[0].set_ylabel('Validation accuracy',fontsize=15)\n",
    "axs[0].set_xlim([-0.3, 1.3])\n",
    "axs[0].set_ylim([-0.3, 1.3])\n",
    "\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.4)\n",
    "textstr = 'PCC: 0.48 \\n'+r'P-value: 5.3$\\times 10^{-3}$'\n",
    "axs[0].text(0.52, 0.15, textstr, transform=axs[0].transAxes, fontsize=14, verticalalignment='top', bbox=props)\n",
    "\n",
    "sns.kdeplot(x=rank_list, y=rob_acc_list,fill=True,ax=axs[1],color='gray',thresh=.05)\n",
    "axs[1].set_xlabel('Rank of Jacobian \\n (b)',fontsize=15)\n",
    "axs[1].set_ylabel('Robust validation accuracy',fontsize=15)\n",
    "axs[1].set_xlim([-0.3, 1.3])\n",
    "axs[1].set_ylim([-0.3, 1.3])\n",
    "\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.4)\n",
    "textstr = 'PCC: 0.51 \\n'+r'P-value: 3.1$\\times 10^{-3}$'\n",
    "axs[1].text(0.52, 0.15, textstr, transform=axs[1].transAxes, fontsize=14, verticalalignment='top', bbox=props)\n",
    "\n",
    "sns.kdeplot(x=pbn_list, y=acc_list,fill=True,ax=axs[2], thresh=.05)\n",
    "axs[2].set_xlabel('Pull-back norm \\n (c)',fontsize=15)\n",
    "axs[2].set_ylabel('Validation accuracy',fontsize=15)\n",
    "axs[2].set_xlim([-0.3, 1.3])\n",
    "axs[2].set_ylim([-0.3, 1.3])\n",
    "\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.4)\n",
    "textstr = 'PCC: 0.83 \\n'+r'P-value: 3.4$\\times 10^{-9}$'\n",
    "axs[2].text(0.52, 0.15, textstr, transform=axs[2].transAxes, fontsize=14, verticalalignment='top', bbox=props)\n",
    "\n",
    "\n",
    "\n",
    "sns.kdeplot(x=pbn_list, y=rob_acc_list,fill=True,ax=axs[3],thresh=.05)\n",
    "axs[3].set_xlabel('Pull-back norm \\n (d)',fontsize=15)\n",
    "axs[3].set_ylabel('Robust validation accuracy',fontsize=15)\n",
    "axs[3].set_xlim([-0.3, 1.3])\n",
    "axs[3].set_ylim([-0.3, 1.3])\n",
    "\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.4)\n",
    "textstr = 'PCC: 0.80 \\n'+r'P-value: 2.8$\\times 10^{-8}$'\n",
    "axs[3].text(0.52, 0.15, textstr, transform=axs[3].transAxes, fontsize=14, verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('correlation.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CNN ###\n",
    "\n",
    "correlation_coeff, p_value = pearsonr(pbn_list, acc_list_cnn)\n",
    "print(correlation_coeff, p_value)\n",
    "\n",
    "correlation_coeff, p_value = pearsonr(pbn_list, rob_acc_list_cnn)\n",
    "print(correlation_coeff, p_value)\n",
    "\n",
    "correlation_coeff, p_value = pearsonr(rank_list, acc_list_cnn)\n",
    "print(correlation_coeff, p_value)\n",
    "\n",
    "correlation_coeff, p_value = pearsonr(rank_list, rob_acc_list_cnn)\n",
    "print(correlation_coeff, p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,4,figsize=(20,5))\n",
    "\n",
    "sns.kdeplot(x=rank_list, y=acc_list_cnn,fill=True,ax=axs[0],color='gray',thresh=.05)\n",
    "axs[0].set_xlabel('Rank of Jacobian \\n (a)',fontsize=15)\n",
    "axs[0].set_ylabel('Validation accuracy',fontsize=15)\n",
    "axs[0].set_xlim([-0.3, 1.3])\n",
    "axs[0].set_ylim([-0.3, 1.3])\n",
    "ax = axs[0]\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.4)\n",
    "textstr = 'PCC: 0.84 \\n'+r'P-value: 2.4$\\times 10^{-9}$'\n",
    "ax.text(0.52, 0.15, textstr, transform=ax.transAxes, fontsize=14, verticalalignment='top', bbox=props)\n",
    "\n",
    "\n",
    "\n",
    "sns.kdeplot(x=rank_list, y=rob_acc_list_cnn,fill=True,ax=axs[1],color='gray',thresh=.05)\n",
    "axs[1].set_xlabel('Rank of Jacobian \\n (b)',fontsize=15)\n",
    "axs[1].set_ylabel('Robust validation accuracy',fontsize=15)\n",
    "axs[1].set_xlim([-0.3, 1.3])\n",
    "axs[1].set_ylim([-0.3, 1.3])\n",
    "ax = axs[1]\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.4)\n",
    "textstr = 'PCC: 0.72 \\n'+r'P-value: 3.5$\\times 10^{-6}$'\n",
    "ax.text(0.52, 0.15, textstr, transform=ax.transAxes, fontsize=14, verticalalignment='top', bbox=props)\n",
    "\n",
    "\n",
    "sns.kdeplot(x=pbn_list, y=acc_list_cnn,fill=True,ax=axs[2], thresh=.05)\n",
    "axs[2].set_xlabel('Pull-back norm \\n (c)',fontsize=15)\n",
    "axs[2].set_ylabel('Validation accuracy',fontsize=15)\n",
    "axs[2].set_xlim([-0.3, 1.3])\n",
    "axs[2].set_ylim([-0.3, 1.3])\n",
    "ax = axs[2]\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.4)\n",
    "textstr = 'PCC: 0.78 \\n'+r'P-value: 1.5$\\times 10^{-7}$'\n",
    "ax.text(0.52, 0.15, textstr, transform=ax.transAxes, fontsize=14, verticalalignment='top', bbox=props)\n",
    "\n",
    "\n",
    "sns.kdeplot(x=pbn_list, y=rob_acc_list_cnn,fill=True,ax=axs[3],thresh=.05)\n",
    "axs[3].set_xlabel('Pull-back norm \\n (d)',fontsize=15)\n",
    "axs[3].set_ylabel('Robust validation accuracy',fontsize=15)\n",
    "axs[3].set_xlim([-0.3, 1.3])\n",
    "axs[3].set_ylim([-0.3, 1.3])\n",
    "ax = axs[3]\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.4)\n",
    "textstr = 'PCC: 0.72 \\n'+r'P-value: 3.4$\\times 10^{-6}$'\n",
    "ax.text(0.52, 0.15, textstr, transform=ax.transAxes, fontsize=14, verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('correlation_cnn.png',dpi=300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phomology",
   "language": "python",
   "name": "phomology"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
